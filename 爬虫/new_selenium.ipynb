{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "\n",
    "爬取[萌娘百科](https://mzh.moegirl.org.cn/)。从GalGame公司页面和动画公司页面开始。\n",
    "\n",
    "由于使用`action=raw`参数可以获得页面源内容，所以使用`requests`获取页面源内容，同时获取状态码，不是200则不爬取。\n",
    "\n",
    "由于该页面有动态加载部分，为了实现链接分析，获取页面上所有内容，所以使用`selenium`爬取页面内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib.parse import quote,unquote\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSpiderTime = 100000\n",
    "service = Service(\"G:\\Python\\edgedriver_win64\\msedgedriver.exe\")\n",
    "options = webdriver.EdgeOptions()\n",
    "options.add_argument(\"headless\")\n",
    "driver = webdriver.Edge(service=service,options=options)\n",
    "driver.set_page_load_timeout(3)\n",
    "driver.set_script_timeout(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.58\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "# url = \"https://mzh.moegirl.org.cn/index.php?title=Special%3ARandom&utm_source=moe_homeland\"\n",
    "url = [\"https://mzh.moegirl.org.cn/index.php?utm_medium=simple_list&title=Template%3AGalgame%E5%85%AC%E5%8F%B8&utm_source=moe_homeland\",\"https://mzh.moegirl.org.cn/index.php?utm_medium=simple_list&title=Template%3A%E5%8A%A8%E7%94%BB%E5%85%AC%E5%8F%B8&utm_source=moe_homeland\"]\n",
    "url_info = {}\n",
    "urls_not_found = set()\n",
    "urls_company = set()\n",
    "urls_falid = set()\n",
    "urls_pages = set()\n",
    "sleepTime = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次爬取：爬取GalGame、ACG和动画公司信息\n",
    "def first_spider():\n",
    "    for i in url:\n",
    "        try:\n",
    "            getPage = False\n",
    "            while not getPage:\n",
    "                try:\n",
    "                    driver.get(url=i)\n",
    "                except:\n",
    "                    time.sleep(sleepTime)\n",
    "                link_elements = driver.find_elements(By.TAG_NAME,\"a\")\n",
    "                if len(link_elements) != 0:\n",
    "                    getPage = True\n",
    "            for link in link_elements:\n",
    "                link_href = link.get_attribute(\"href\")\n",
    "                if link_href == None or link_href == '':\n",
    "                    continue\n",
    "                one_url = unquote(link_href)\n",
    "                if one_url[0] == '/':\n",
    "                    one_url = \"https://mzh.moegirl.org.cn\" + one_url\n",
    "                try:\n",
    "                    resp = requests.get(url=one_url,headers=request_headers)\n",
    "                except:\n",
    "                    continue\n",
    "                if not resp.status_code == 200:\n",
    "                    urls_not_found.add(one_url)\n",
    "                    continue\n",
    "                urls_company.add(one_url)\n",
    "                if one_url in url_info.keys():\n",
    "                    if link.text.strip() != \"\":\n",
    "                        url_info[one_url][\"anchor_text\"].add(link.text.strip())\n",
    "                else:\n",
    "                    url_info[one_url] = {}\n",
    "                    url_info[one_url][\"url\"] = one_url\n",
    "                    url_info[one_url][\"anchor_text\"] = set()\n",
    "                    url_info[one_url][\"title\"] = \"\"\n",
    "                    url_info[one_url][\"page\"] = 0\n",
    "                    url_info[one_url][\"page_rank\"] = 0\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取网页内容\n",
    "def second_spider(urls, add_link=True, store_link=False):\n",
    "    sleepTime = 0\n",
    "    for i in urls:\n",
    "        try:\n",
    "            try:\n",
    "                driver.get(url=i)\n",
    "            except:\n",
    "                time.sleep(sleepTime)\n",
    "                if i not in url_info.keys():\n",
    "                    url_info[i] = {}\n",
    "                    url_info[i][\"url\"] = i\n",
    "                    url_info[i][\"anchor_text\"] = set()\n",
    "                    url_info[i][\"title\"] = \"\"\n",
    "                    url_info[i][\"page\"] = 0\n",
    "                    url_info[i][\"page_rank\"] = 0\n",
    "                # 获取标题信息\n",
    "                url_info[i][\"title\"] = driver.find_element(By.TAG_NAME,\"h1\").text\n",
    "                # 获取页面内容并写入文件\n",
    "                try:\n",
    "                    content = driver.find_element(By.CSS_SELECTOR,'#mw-body')\n",
    "                    sleepTime = 0\n",
    "                except:\n",
    "                    urls_falid.add(i)\n",
    "                    sleepTime += 1\n",
    "                    continue\n",
    "                timeNow = int(time.mktime(time.localtime(time.time())))\n",
    "                with open('spider_pages/' + str(timeNow), 'w', encoding='utf-8') as f:\n",
    "                    f.write(content.text)\n",
    "                url_info[i][\"page\"] = timeNow\n",
    "                if store_link:\n",
    "                    with open(\"spider_pages/\" + str(timeNow) + \"_to_link\", 'w', encoding='utf-8') as f:\n",
    "                        for link in driver.find_elements(By.TAG_NAME,\"a\"):\n",
    "                            try:\n",
    "                                link_href = link.get_attribute(\"href\")\n",
    "                            except:\n",
    "                                continue\n",
    "                            if link_href == None or link_href == '':\n",
    "                                continue\n",
    "                            one_url = unquote(link_href)\n",
    "                            f.write(one_url)\n",
    "                            f.write('\\n')\n",
    "                if not add_link:\n",
    "                    continue\n",
    "                for link in driver.find_elements(By.TAG_NAME,\"a\"):\n",
    "                    link_href = link.get_attribute(\"href\")\n",
    "                    if link_href == None or link_href == '':\n",
    "                        continue\n",
    "                    one_url = unquote(link_href)\n",
    "                    if one_url[0] == '/':\n",
    "                        one_url = \"https://mzh.moegirl.org.cn\" + one_url\n",
    "                    # 如果是无法获取或者公司的链接，则continue\n",
    "                    if one_url in urls_not_found or one_url in urls_company:\n",
    "                        continue\n",
    "                    urls_pages.add(one_url)\n",
    "                    if one_url in url_info.keys():\n",
    "                        url_info[one_url][\"anchor_text\"].add(link.text.strip())\n",
    "                    else:\n",
    "                        url_info[one_url] = {}\n",
    "                        url_info[one_url][\"url\"] = one_url\n",
    "                        url_info[one_url][\"anchor_text\"] = set()\n",
    "                        url_info[one_url][\"title\"] = \"\"\n",
    "                        url_info[one_url][\"page\"] = 0\n",
    "                        url_info[one_url][\"page_rank\"] = 0\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 加上action=raw，爬取源文件\n",
    "def second_spider_raw(urls, add_link=True, store_link=False, over_write = False):\n",
    "    sleepTime = 0\n",
    "    for i in urls:\n",
    "        try:\n",
    "            split_url = re.split('[#?&]', i)\n",
    "            resp = requests.get(url=split_url[0] + \"?action=raw\",headers=request_headers)\n",
    "            content = resp.text\n",
    "            sleepTime = 0\n",
    "        except:\n",
    "            urls_falid.add(i)\n",
    "            sleepTime += 1\n",
    "            continue\n",
    "        if resp.status_code != 200:\n",
    "            continue\n",
    "        if not over_write:\n",
    "            if i in url_info.keys() and url_info[i][\"page\"] != 0:\n",
    "                continue\n",
    "        try:\n",
    "            try:\n",
    "                driver.get(url=i)\n",
    "            except:\n",
    "                time.sleep(sleepTime)\n",
    "                if i not in url_info.keys():\n",
    "                    url_info[i] = {}\n",
    "                    url_info[i][\"url\"] = i\n",
    "                    url_info[i][\"anchor_text\"] = set()\n",
    "                    url_info[i][\"title\"] = \"\"\n",
    "                    url_info[i][\"page\"] = 0\n",
    "                    url_info[i][\"page_rank\"] = 0\n",
    "                # 获取标题信息\n",
    "                url_info[i][\"title\"] = driver.find_element(By.TAG_NAME,\"h1\").text\n",
    "                timeNow = int(time.mktime(time.localtime(time.time())))\n",
    "                with open('spider_pages/' + str(timeNow), 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                url_info[i][\"page\"] = timeNow\n",
    "                if store_link:\n",
    "                    with open(\"spider_pages/\" + str(timeNow) + \"_to_link\", 'w', encoding='utf-8') as f:\n",
    "                        for link in driver.find_elements(By.TAG_NAME,\"a\"):\n",
    "                            try:\n",
    "                                link_href = link.get_attribute(\"href\")\n",
    "                            except:\n",
    "                                continue\n",
    "                            if link_href == None or link_href == '':\n",
    "                                continue\n",
    "                            one_url = unquote(link_href)\n",
    "                            res = re.split('[#?&]', one_url)\n",
    "                            f.write(res[0])\n",
    "                            f.write('\\n')\n",
    "                if not add_link:\n",
    "                    continue\n",
    "                for link in driver.find_elements(By.TAG_NAME,\"a\"):\n",
    "                    link_href = link.get_attribute(\"href\")\n",
    "                    if link_href == None or link_href == '':\n",
    "                        continue\n",
    "                    one_url = unquote(link_href)\n",
    "                    if one_url[0] == '/':\n",
    "                        one_url = \"https://mzh.moegirl.org.cn\" + one_url\n",
    "                    res = re.split('[#?&]', one_url)\n",
    "                    one_url = res[0]\n",
    "                    # 如果是无法获取或者公司的链接，则continue\n",
    "                    if one_url in urls_not_found or one_url in urls_company:\n",
    "                        continue\n",
    "                    urls_pages.add(one_url)\n",
    "                    if one_url in url_info.keys():\n",
    "                        url_info[one_url][\"anchor_text\"].add(link.text.strip())\n",
    "                    else:\n",
    "                        url_info[one_url] = {}\n",
    "                        url_info[one_url][\"url\"] = one_url\n",
    "                        url_info[one_url][\"anchor_text\"] = set()\n",
    "                        url_info[one_url][\"title\"] = \"\"\n",
    "                        url_info[one_url][\"page\"] = 0\n",
    "                        url_info[one_url][\"page_rank\"] = 0\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一次爬虫\n",
    "\n",
    "从两个页面爬取GalGame公司和动画公司信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取文件\n",
    "\n",
    "由于在爬虫时需要保存数据，所以有的时候会需要从保存的数据中读取信息，以下为读取信息的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"page_info.json\", \"r\", encoding='utf-8') as f:\n",
    "    url_info = json.load(f)\n",
    "    for key,value in url_info.items():\n",
    "        url_info[key][\"anchor_text\"] = set(value[\"anchor_text\"])\n",
    "\n",
    "# with open(\"urls_company.pk\",\"rb\") as f:\n",
    "#     urls_company = pickle.load(f)\n",
    "\n",
    "# with open(\"urls_not_found.pk\", \"rb\") as f:\n",
    "#     urls_not_found = pickle.load(f)\n",
    "\n",
    "# with open(\"urls_pages.pk\", \"rb\") as f:\n",
    "#     urls_pages = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写入文件\n",
    "\n",
    "第一次爬取信息时需要写入文件，备份。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"page_info.json\", \"w\", encoding='utf-8') as f:\n",
    "#     for key,value in url_info.items():\n",
    "#         url_info[key][\"anchor_text\"] = list(value[\"anchor_text\"])\n",
    "#     json.dump(url_info, f, indent=4, ensure_ascii=False)\n",
    "#     for key,value in url_info.items():\n",
    "#         url_info[key][\"anchor_text\"] = set(value[\"anchor_text\"])\n",
    "\n",
    "# with open(\"urls_company.pk\",\"wb\") as f:\n",
    "#     pickle.dump(urls_company, f)\n",
    "\n",
    "# with open(\"urls_not_found.pk\", \"wb\") as f:\n",
    "#     pickle.dump(urls_not_found, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除其他站点的数据\n",
    "\n",
    "每轮爬取结束后都需要删除其他站点数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_other_link():\n",
    "    keys = list(url_info.keys())\n",
    "    for i in keys:\n",
    "        if re.match(r'https://mzh.moegirl.org.cn/.*', i):\n",
    "            continue\n",
    "        else:\n",
    "            p = url_info.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18367"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_other_link()\n",
    "len(url_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"page_info.json\", \"w\", encoding='utf-8') as f:\n",
    "#     for key,value in url_info.items():\n",
    "#         url_info[key][\"anchor_text\"] = list(value[\"anchor_text\"])\n",
    "#     json.dump(url_info, f, indent=4, ensure_ascii=False)\n",
    "#     for key,value in url_info.items():\n",
    "#         url_info[key][\"anchor_text\"] = set(value[\"anchor_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二次爬取\n",
    "\n",
    "第一次爬取之后，获得了465个公司的信息，第二次需要在每个公司页面爬取链接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_spider(urls_company, store_link=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_spider(urls_falid, store_link=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"urls_pages.pk\", \"wb\") as f:\n",
    "#     pickle.dump(urls_pages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# testpage = random.choice(list(urls_pages))\n",
    "# second_spider([testpage], False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20310"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url_info_new = {}\n",
    "# for key,value in url_info.items():\n",
    "#     res = re.split('[#?&]', key)\n",
    "#     if len(res) == 0:\n",
    "#         url_info_new[key] = value\n",
    "#     else:\n",
    "#         match = re.match(r'https://mzh.moegirl.org.cn/index.php\\?title=.*', key)\n",
    "#         if match:\n",
    "#             continue\n",
    "#         else:\n",
    "#             url_info_new[res[0]] = value\n",
    "#             url_info_new[res[0]][\"url\"] = res[0]\n",
    "\n",
    "# len(url_info_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "备份数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"page_info.json\", \"w\", encoding='utf-8') as f:\n",
    "#     for key,value in url_info.items():\n",
    "#         url_info[key][\"anchor_text\"] = list(value[\"anchor_text\"])\n",
    "#     json.dump(url_info, f, indent=4, ensure_ascii=False)\n",
    "#     for key,value in url_info.items():\n",
    "#         url_info[key][\"anchor_text\"] = set(value[\"anchor_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三次爬取\n",
    "\n",
    "对于第二次获取到的所有公司的作品，进行第三次爬取，保存页面内容和页面中的链接，但是不增加爬取链接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取之前处理\n",
    "\n",
    "由于爬取的四万余链接中，大部分为无效链接，所以对链接进行预处理。\n",
    "\n",
    "设定过滤链接，移除所有与过滤链接匹配的链接，以正则表达式给出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15963"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_filter = [\n",
    "    r\"https://mzh.moegirl.org.cn/Special:.*\", \n",
    "    r\"https://mzh.moegirl.org.cn/index.php?title=.*\", \n",
    "    r\".*\\.jpg\", \n",
    "    r\".*\\.png\", \n",
    "    r\"https://mzh.moegirl.org.cn/萌娘百科:.*\",\n",
    "    r\"https://mzh.moegirl.org.cn/萌娘百科_talk:\"\n",
    "    ]\n",
    "\n",
    "keys = list(url_info.keys())\n",
    "\n",
    "for key in keys:\n",
    "    for i in url_filter:\n",
    "        if re.match(i, key):\n",
    "            m = url_info.pop(key)\n",
    "\n",
    "len(url_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"page_info_new.json\", \"w\", encoding='utf-8') as f:\n",
    "#     for key,value in url_info_new.items():\n",
    "#         url_info_new[key][\"anchor_text\"] = list(value[\"anchor_text\"])\n",
    "#     json.dump(url_info_new, f, indent=4, ensure_ascii=False)\n",
    "#     for key,value in url_info_new.items():\n",
    "#         url_info_new[key][\"anchor_text\"] = set(value[\"anchor_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_pages = list(url_info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于多次中断了爬虫，所以在总的链接中会有已爬取的和未爬取的链接，需要将未爬取的链接取出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_not_search = []\n",
    "for i in urls_pages:\n",
    "    if url_info[i][\"page\"] == 0:\n",
    "        url_not_search.append(i)\n",
    "\n",
    "len(url_not_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"url_not_search.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(url_not_search, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://mzh.moegirl.org.cn/Template:Galgame公司',\n",
       " 'https://mzh.moegirl.org.cn/COOL',\n",
       " 'https://mzh.moegirl.org.cn/松竹',\n",
       " 'https://mzh.moegirl.org.cn/糖果盒动画',\n",
       " 'https://mzh.moegirl.org.cn/童梦',\n",
       " 'https://mzh.moegirl.org.cn/晓(公司)',\n",
       " 'https://mzh.moegirl.org.cn/亚细亚堂',\n",
       " 'https://mzh.moegirl.org.cn/月虹(公司)',\n",
       " 'https://mzh.moegirl.org.cn/朱夏',\n",
       " 'https://mzh.moegirl.org.cn/ALCOO']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_not_search[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按1000个链接分组，每跑完一组就保存一次链接和页面的信息（`page_info`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(url_not_search)\n",
    "for i in range(0, total, 1000):\n",
    "    next = i + 1000\n",
    "    if next > total:\n",
    "        next = total\n",
    "    second_spider_raw(url_not_search[i:next], False, True)\n",
    "    with open(\"page_info{}.json\".format(i), \"w\", encoding='utf-8') as f:\n",
    "        for key,value in url_info.items():\n",
    "            url_info[key][\"anchor_text\"] = list(value[\"anchor_text\"])\n",
    "            if \"\" in url_info[key][\"anchor_text\"]:\n",
    "                url_info[key][\"anchor_text\"].remove(\"\")\n",
    "        json.dump(url_info, f, indent=4, ensure_ascii=False)\n",
    "        for key,value in url_info.items():\n",
    "            url_info[key][\"anchor_text\"] = set(value[\"anchor_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取完成后处理\n",
    "\n",
    "爬取完成后，将所有无法爬取的链接从字典中移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17484"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(url_info.keys())\n",
    "for key in keys:\n",
    "    if url_info[key][\"page\"] == 0:\n",
    "        m = url_info.pop(key)\n",
    "\n",
    "len(url_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"page_info.json\", \"w\", encoding='utf-8') as f:\n",
    "    for key,value in url_info.items():\n",
    "        url_info[key][\"anchor_text\"] = list(value[\"anchor_text\"])\n",
    "    json.dump(url_info, f, indent=4, ensure_ascii=False)\n",
    "    for key,value in url_info.items():\n",
    "        url_info[key][\"anchor_text\"] = set(value[\"anchor_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"urls_pages.pk\", \"wb\") as f:\n",
    "    pickle.dump(urls_pages, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理页面\n",
    "\n",
    "在爬取页面过程中，可能会出现页面不被需要的情况，所以将所有有用的页面复制到另一个文件夹中。\n",
    "\n",
    "页面处理分为两方面：\n",
    "\n",
    "+ 将所有属于最后链接对应的页面移动到另一个文件夹中。\n",
    "+ 读取页面链接，将处理我们爬取的链接的链接写入另一个文件夹中的对应文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for key, value in url_info.items():\n",
    "    page_name = str(value[\"page\"])\n",
    "    m = shutil.copy(\"spider_pages/\" + page_name, \"new_pages/\" + page_name)\n",
    "    \n",
    "    try:\n",
    "        with open(\"spider_pages/\" + page_name + \"_to_link\", \"r\", encoding='utf-8') as f:\n",
    "            to_urls = f.readlines()\n",
    "    except:\n",
    "        with open(\"page_link_num/\" + page_name, \"w\", encoding='utf-8') as f:\n",
    "            m = json.dump([], f, indent=4, ensure_ascii=False)\n",
    "        continue\n",
    "\n",
    "    to_urls_new = set()\n",
    "    for i in to_urls:\n",
    "        res = re.split('[#?&]', i)\n",
    "        add_url = res[0].strip()\n",
    "        if add_url in url_info.keys() and add_url != \"https://mzh.moegirl.org.cn/index.php\":\n",
    "            to_urls_new.add(url_info[add_url][\"page\"])\n",
    "    \n",
    "    with open(\"page_link_num/\" + page_name, \"w\", encoding='utf-8') as f:\n",
    "        m = json.dump(list(to_urls_new), f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 链接分析\n",
    "\n",
    "为了后续链接分析的方便，我们在这里构建链接关系图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从链接关系构建链接图，保存图中的所有边，为`link_graph.pk`。保存格式为\n",
    "```\n",
    "{\n",
    "    (int)from1:[(int)to1, (int)to2,...],\n",
    "    (int)from2:[(int)to3, (int)to4,...],\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15963"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_edges = os.listdir(\"page_link_num\")\n",
    "len(link_edges)\n",
    "link_graph = {}\n",
    "for i in link_edges:\n",
    "    with open(\"page_link_num/\" + i, \"r\", encoding=\"utf-8\") as f:\n",
    "        edges = json.load(f)\n",
    "    link_graph[int(i)] = edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"link_graph.pk\", \"wb\") as f:\n",
    "    pickle.dump(link_graph, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便，我们设计`num2link.pk`文件，构建从页面到链接的映射关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2link = {}\n",
    "for key, value in url_info.items():\n",
    "    num2link[value[\"page\"]] = key\n",
    "\n",
    "with open(\"num2link.pk\", \"wb\") as f:\n",
    "    pickle.dump(num2link, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
